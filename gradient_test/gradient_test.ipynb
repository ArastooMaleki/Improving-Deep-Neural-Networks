{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient test\n",
    "in this notebook we get to know a test witch help us to findout our implementation of \n",
    "backpropagation is correct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple function \n",
    "lets strat with a simple 2d function J = thetha * x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from testCases import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x,theta):\n",
    "    return np.dot(theta,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x,theta):\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n"
     ]
    }
   ],
   "source": [
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x,theta,epsilon=1e-7):\n",
    "    J_plus  = forward_propagation(x,theta+epsilon)\n",
    "    J_minus = forward_propagation(x,theta-epsilon)\n",
    "    gradapprox = (J_plus - J_minus)/(2*epsilon)\n",
    "    grad  = backward_propagation(x,theta)\n",
    "    diffrence = np.divide((np.linalg.norm(grad - gradapprox)),(np.linalg.norm(grad) + np.linalg.norm(gradapprox)))\n",
    "    if diffrence < epsilon:\n",
    "        print(\"gradient is correct\")\n",
    "    else:\n",
    "        print(\"gradient is wrong !\")\n",
    "    return diffrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient is correct\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n dimension function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gc_utils import relu , sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X,y,param):\n",
    "    \"\"\"\n",
    "    implementation of n dimension function like network\n",
    "    return:\n",
    "    cost -- loss over whole training set \n",
    "    cache -- all the information of net in each layer in order of Z , A , W , b\n",
    "    *** as you can see this dosnt return any y_hat\n",
    "    \"\"\"\n",
    "    # initializing \n",
    "    W1 = param[\"W1\"]\n",
    "    b1 = param[\"b1\"]\n",
    "    W2 = param[\"W2\"]\n",
    "    b2 = param[\"b2\"]\n",
    "    W3 = param[\"W3\"]\n",
    "    b3 = param[\"b3\"]\n",
    "    # forward pass\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3,A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    # compute cost\n",
    "    try:\n",
    "        m = y.shape[1]\n",
    "    except:\n",
    "        m = len(y)\n",
    "        \n",
    "    \n",
    "    \n",
    "    log_prob = -(y*np.log(A3))-((1-y)*np.log((1-A3)))\n",
    "    cost = np.sum(log_prob) / m\n",
    "\n",
    "    cache_ = (Z1 , A1 , W1 ,b1 , Z2 ,A2 , W2 , b2 ,Z3 , A3 ,W3 , b3)\n",
    "    return cost , cache_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X,y,cache):\n",
    "    \"\"\"\n",
    "    implementation of n dimension function like net \n",
    "\n",
    "    arguments:\n",
    "    X -- feature matrix \n",
    "    y -- ground true label\n",
    "    cache -- cache from forward pass\n",
    "    \n",
    "    return:\n",
    "    grad -- dictionary of gradiant\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization \n",
    "    try:\n",
    "        m = y.shape[1]\n",
    "    except:\n",
    "        m = len(y)\n",
    "    (Z1 , A1 , W1 ,b1 , Z2 ,A2 , W2 , b2 ,Z3 , A3 ,W3 , b3) = cache\n",
    "    \n",
    "    # backward pass \n",
    "    dZ3 = A3 - y\n",
    "    dW3 = np.dot(dZ3,A2.T)/m \n",
    "    db3 = np.sum(dZ3,axis=1,keepdims=True)/m\n",
    "    dA2 = np.dot(W3.T,dZ3)\n",
    "    dZ2 = np.multiply(dA2,np.int64(A2>0))\n",
    "    dW2 = np.dot(dZ2,A1.T)/m \n",
    "    db2 = np.sum(dZ2,axis=1,keepdims=True)/m \n",
    "    dA1 = np.dot(W2.T,dZ2)\n",
    "    dZ1 = np.multiply(dA1,np.int64(A1>0))\n",
    "    dW1 = np.dot(dZ1,X.T) /m \n",
    "    db1 = np.sum(dZ1,axis=1,keepdims=True)/m \n",
    "\n",
    "    grads = {\n",
    "        \"dW3\":dW3 , \"db3\":db3 , \"dZ3\":dZ3 , \"dA2\":dA2 ,\n",
    "        \"dW2\":dW2 , \"db2\":db2 , \"dZ2\":dZ2 , \"dA1\":dA2 ,\n",
    "        \"dW1\":dW1 , \"db1\":db1 , \"dZ1\":dZ1 \n",
    "    }\n",
    "\n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gc_utils import vector_to_dictionary , dictionary_to_vector , gradients_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(param,grad,X,y,epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    implement gradient check for network with matrix and vector weights\n",
    "\n",
    "    arguments:\n",
    "    param -- network parameters (during training or after training)\n",
    "    grad -- our created gradiants \n",
    "    X -- feature matrix \n",
    "    y -- ground true labels\n",
    "    epsilon -- lenth of derivative \n",
    "\n",
    "    returns:\n",
    "    diffrence -- a value witch show how our gradient is ok \n",
    "\n",
    "    \"\"\"\n",
    "    # create theta \n",
    "    theta  , _ = dictionary_to_vector(param)\n",
    "    theta_plus = np.copy(theta) #+ epsilon\n",
    "    theta_minus = np.copy(theta) #- epsilon \n",
    "    # craete gradapprox\n",
    "    gradapprox = np.zeros(theta.shape)\n",
    "    for i in range(theta.shape[0]):\n",
    "        # theta_plus = np.copy(theta) #+ epsilon\n",
    "        # theta_minus = np.copy(theta) #- epsilon\n",
    "        theta_plus[i][0] = theta[i][0] + epsilon\n",
    "        theta_minus[i][0] = theta[i][0] - epsilon\n",
    "\n",
    "        J_plus , cache_plus = forward_propagation_n(X,y,vector_to_dictionary(theta_plus))\n",
    "        J_minus , cache_minus = forward_propagation_n(X,y,vector_to_dictionary(theta_minus))\n",
    "        gradapprox[i][0] = (J_plus - J_minus) / (2*epsilon)\n",
    "\n",
    "        theta_plus[i][0] = theta[i][0] \n",
    "        theta_minus[i][0] = theta[i][0] \n",
    "    \n",
    "    grad_vectore = gradients_to_vector(grad)\n",
    "    numerator = np.linalg.norm(grad_vectore - gradapprox)\n",
    "    denumerator = np.linalg.norm(grad_vectore) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denumerator\n",
    "\n",
    "    if difference <= 2*epsilon:\n",
    "        print(\"gradiants is ok\")\n",
    "        print(difference)\n",
    "    else:\n",
    "        print(\"gradiants is wrong!\")\n",
    "        print(difference)\n",
    "\n",
    "    return difference\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradiants is ok\n",
      "1.1909939370580298e-07\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
